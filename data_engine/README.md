# Data Engine

## Overview

This module constructs a DPO dataset for direct training. Simply input your reward model, instruct model, and
dataset, then run the `run_engine.sh` script to generate the dataset.

- **Instruct Model**: Used to generate raw answers to questions in the dataset.
- **Reward Model**: Evaluates the answers generated by the instruct model, providing rewards to rank them and build
  the DPO dataset.

## Usage

Refer to the `run_engine.sh` script for execution. Two pipelines are available for data construction:

1. **Divide and Conquer Pipeline** (`divide_and_conquer`)
2. **DPO Reward Pipeline** (`dpo_reward`)
3. **LLaVA Critic Pipeline** (`llava_critic`)

Each pipeline has distinct requirements, explained below.

---

### Divide and Conquer Pipeline

#### Process Method

Use RLAIF-V divide-and-conquer strategy to collect AI feedback.

#### Required Models

- **Split Model**: [Download here](https://thunlp.oss-cn-qingdao.aliyuncs.com/rlaifv_llama3_split_model.tar.gz)
- **Question Transformation Model**: [Download here](https://thunlp.oss-cn-qingdao.aliyuncs.com/rlaifv_llama3_changeq_model.tar.gz)

#### Custom Implementation

We support `llava-1.5-7b` as the instruct model. For the reward model, three models are required:

1. **Question Change Model** (`rlaifv_llama3_changeq_model`)
2. **Question Split Model** (`rlaifv_llama3_split_model`)
3. **Auto Check Model** (`OmniLMM-12B` or `MiniCPM-Llama3-V-2_5`)

We don't recommend you to change Question Change Model and Question Split Model.If you wish to use other models as
instruct model or auto check model, custom implementation is needed:

1. **Generate Rollouts**
    - Needed if you want new instruct model
    - Add a model builder in `RLAIF-V/builder`.
    - Add corresponding caller code in `RLAIF-V/builder/builder.py`.
    - Refer to `RLAIF-V/muffin/llava15_gen_data.py` for sampling logic.

2. **Reward Collection**
    - Needed if you want new auto check model
    - Implement builder.
    - Update `reward_calculate` function in
      `RLAIF-V/data_engine/pipeline/divide_and_conquer/divide_and_conquer_pipeline.py` to call your own model.

#### Dataset Format

Dataset should be in `.jsonl` format with the following fields:

- `question`: The image-related question.
- `question_id`: Optional.
- `image`: Base64-encoded binary data (optional if `image_path` is present).
- `image_path`: Path to the image (optional if `image` is present). If `image` is empty, `image_path` will be used.

#### Script Parameters

- `--reward_model_path`: Comma-separated paths to the auto check model, question change model, and split model (e.g.,
  `/path/to/MiniCPM-Llama3-V-2_5,/path/to/changeq_model,/path/to/split_model`).
- `--reward_model_name`: Name of the auto check model (e.g., `MiniCPM-Llama3-V-2_5`).
- `--pipeline_name`: `divide_and_conquer`.
- `--reward_model_python_path`: Python path for configuring the auto check model environment (required for MiniCPM-V).

---

### DPO Reward Pipeline

#### Process Method

Use RLAIF-V self-feedback guidance with DPO-trained models.

#### Custom Implementation

Supported models:

- Instruct Models: `llava-1.5-7b`, `OmniLMM-12B`, `MiniCPM-V-2_6` (need to be run under MiniCPM-V-2_6 environment)
- Reward Models: `RLAIF-V-7B`, `RLAIF-V-12B`

For other models, implement custom code:

1. **Generate Rollouts**
    - Needed for new instruct model.
    - Add model builders in `RLAIF-V/builder`.
    - Add caller code in `RLAIF-V/builder/builder.py`.
    - Refer to `RLAIF-V/llava/llava15_sample_data.py` for sampling logic.
    - Update `RLAIF-V/data_engine/pipeline/dpo_reward_pipeline/answer_sampler.py`.

2. **Reward Collection**
    - Needed for new reward model.
    - Add model builder.
    - Update `RLAIF-V/data_engine/pipeline/dpo_reward_pipeline/logps_calculator.py` and
      `RLAIF-V/muffin/eval/muffin_inference_logp.py` to ensure compatibility.

#### Dataset Format

Recommended format: `.parquet`  
Fields:

- `idx`: Unique identifier for each entry (string format allowed).
- `question`: Image-related question.
- `image`: Dictionary with keys:
    - `bytes`: Binary format.
    - `path`: Optional but recommended.

#### Script Parameters

- `--pipeline_name`: `dpo_reward`.

---

### LLaVA Critic Pipeline

#### Process Method

Use LLaVA critic model to pairwise judge which answer is better. Then the judge result is used as the score to rank the
answers which is then built into chosen and rejected pairs.

#### Custom Implementation

Supported models:

- Instruct Models: `llava-1.5-7b`, `OmniLMM-12B`, `MiniCPM-V-2_6` (need to be run under MiniCPM-V-2_6 environment)
- Reward Models: `LLaVA-Critic`

For other models, implement custom code:

1. **Generate Rollouts**
    - Needed for new instruct model.
    - Add model builders in `RLAIF-V/builder`.
    - Add caller code in `RLAIF-V/builder/builder.py`.
    - Refer to `RLAIF-V/llava/llava15_sample_data.py` for sampling logic.
    - Update `RLAIF-V/data_engine/pipeline/dpo_reward_pipeline/answer_sampler.py`.

2. **Reward Collection**
    - In this pipeline, we only support LLaVA-Critic as the reward model.

#### Dataset Format

The same as required in the DPO Reward Pipeline.

#### Script Parameters

- `--pipeline_name`: `dpo_reward`.
- `reward_python_path`: conda environment **NAME** for LLaVA-Critic. Besides, if your conda `activate` path is not
  `/opt/conda/bin/activate`, please modify line 14 in the file
  `RLAIF-V/script/data_gen/llava_critic/llava_critic_gen.sh`.

---

### Common Usage Notes

- Specify `--work_dir` to store intermediate and final outputs under a designated directory.
- Use `--debug` for detailed intermediate outputs saved under a `debug` directory.
- If errors occur, use `--run_stage` to rerun specific steps after resolving issues.
- The final dataset path will be displayed upon completion.

---

### Running the Script

```bash
sh data_engine/run_data_engine.sh
```
