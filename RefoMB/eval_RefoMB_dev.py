import re
import os
import time
import tqdm
import glob
import json
import pathlib
import argparse
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import random
from datetime import datetime

from gpt4_api import get_eval4

SYSTEM_MSG = '''
There are currently two multimodal models that urgently need evaluation. We greatly need you to act as an impartial judge and provide valuable evaluation opinions. Only after this can these two models continue to be used. Please conduct a comprehensive and detailed evaluation according to the following requirements to prevent them from being discarded. If your judgment is rich, and high-quality, you can also receive one million dollars.

You need to carefully evaluate the quality of the responses provided by the two multimodal models to users' questions about pictures. Your evaluation is mainly based on the trustworthiness and overall helpfulness of the answer:

* The trustworthiness is measured by the number of hallucinations in the answer. In this context, hallucinations refer to situations where the responses generated by the multimodal models contain information that conflicts with the image description, or information that does not exist in the image description.
* The helpfulness is measured by how effectively the model assists users in achieving their goals by providing accurate, relevant and easy-to-understand information.

Please try to find all the hallucinations in the response. For each additional hallucination you find, an extra tip of one hundred thousand dollars will be paid to you. To check the number of image hallucinations, you need to compare the model's response with the image description, and observe whether there are:
1. Errors in the description of image visual information (including but not limited to types of elements appearing, gender, type of clothing, direction of face and body, actions, positional relationships, text, color, relative size, number of people and objects, identity of characters, age, activities involved, function of items, etc.)
2. Errors in the description of image meta-properties (including but not limited to the environment in which the image was taken, the type of image, the purpose of the image, the quality of the image, the degree of blur of the image, the location of the image in the real or virtual world, etc.)
3. Errors in the metaphorical description of the image (including but not limited to the atmosphere portrayed in the image, viewing experience, the meaning conveyed by the elements in the image, etc.)
4. Other incorrect statements of details not based on the image description.

Please note that the description of the picture already cover all the information of the picture. 
When the question is with creative content, such as being to write a story, the responses can be somewhat creative.
You will make a judgment on the responses of the two models based on the above information.

When you output your evaluation opinions to users, we hope you strictly follow the following format: First, analyze which model is better in terms of accuracy. You need to compare each model's response with the image description and reference information, and find the number of hallucinations. Secondly, analyze which model is better in terms of helpfulness.  Finally, combine accuracy and helpfulness to answer which model you think is better, and strictly output your final conclusion in the following format: 
If Model A is better, output \"[[A]]\"; 
If Model B is better, output \"[[B]]\"; 
If both models are equally good, output \"[[C]]\". 

Now, please make your assessment based on the following information:

'''

def construct_gpt4_query(text_instruction,
                        image_descriptsion,
                        modelA_answer, modelB_answer):
    prompt = f'''
    {SYSTEM_MSG}

    [Beginning of the detailed description of the picture]
    {image_descriptsion}
    [End of the detailed description of the picture]

    [Beginning of the user's question]
    {text_instruction}
    [End of the user's question]

    [Beginning of Model A's answer]
    {modelA_answer}   
    [End of Model A's answer]

    [Beginning of Model B's answer]
    {modelB_answer}
    [End of Model B's answer]
    '''
    return prompt




def post_process(output):
    match = re.findall(r'\[\[(A|B|C)\]\]', output)[0]

    if 'A' in match:
        score = -1
    elif 'B' in match:
        score = 1
    elif 'C' in match:
        score = 0
    else:
        score = 2

    review = output
    return score, review



def read_jsonl_modelA_0504(file_path):
    data_dict = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            try:
                item = json.loads(line.strip()) 
            except json.JSONDecodeError as e:
                print(f"JSONDecodeError: {e}" )
            data_dict.append(
                {
                 'image_url' : item['image_url'],
                 'question' : item['question'],
                 'description' : item['description'],
                 'type' : item['type'],
                 'split': item['split'],
                 'modelA answer' : item['answer']
                 })
    return data_dict


def read_jsonl_modelB_0504(file_path):
    data_dict = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            item = json.loads(line.strip())
            data_dict.append(
                {
                 'image_url' : item['image_url'],
                 'type' : item['type'],
                 'question' : item['question'],
                 'modelB answer' : item['answer']
                 })
    return data_dict

def merge_modeA_modelB_0504(list1, list2):
    for dict1 in list1:
        for dict2 in list2:
            if dict1['question'] == dict2['question'] and dict1['image_url'] == dict2['image_url']:
                dict1['modelB answer'] = dict2['modelB answer']
                

def check_keys(data_list):
    for index, item in enumerate(data_list):
        if 'modelB answer' not in item:
            raise Exception(f"Key 'modelB answer' is missing in the dictionary at index {index}")
        else:
            pass
            

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='OpenMME evaluation')
    parser.add_argument('--answer_modelA', type=str,
                        default='./eval/data/RefoMB_gpt4v_dev.jsonl')
    parser.add_argument('--answer_modelB', type=str,
                        default='xxxx')

    args = parser.parse_args()

    ref_model_list = ['GPT-4V']
    
    file1_data_modelA = read_jsonl_modelA_0504(args.answer_modelA)

    file2_data_modelB = read_jsonl_modelB_0504(args.answer_modelB)

    merge_modeA_modelB_0504(file1_data_modelA, file2_data_modelB)
    
    try:
        check_keys(file1_data_modelA)
    except Exception as e:
        print(e)

    image_path_list =[]
    question_list =[]
    description_list = []
    ref_answer_modelA =[]
    type_name_list = [] 
    answer_modelB =[]

    file1_data_modelA = file1_data_modelA[0:2]

    for item in file1_data_modelA:
        
        image_path_list.append(item['image_url'])
        question_list.append(item['question'])
        description_list.append(item['description'])
        ref_answer_modelA.append(item['modelA answer'])
        answer_modelB.append(item['modelB answer'])
        type_name_list.append(item['type'])

    modelB = args.answer_modelB.split("openmme_0511_eval/")[1]
    
    print(f'Evaluating {args.answer_modelB}')
        
    reviews = []

    with ThreadPoolExecutor(max_workers=64) as executor:
            tasks = []
            modelname = []
            imageid = []
            token = 0
            in_token = 0
            out_token = 0
            def eval_worker(x,modelA,modelB,qid, type_name,image_path):
                while True:
                    response = get_eval4(x)
                    try:
                        score, review = post_process(response)
                        out = {
                            'score': score,
                            'review': review,
                            'prompt': x,
                            'image': qid,
                            'modelA': modelA,
                            'modelB': modelB,
                            'type_name': type_name,
                            'image_path_list': image_path,
                        }
                        return out
                    except:
                        print(f'Fail parsing {response}')
                        continue

            for qid, (text_instruction) in enumerate(zip(question_list)):
                modelA = ref_model_list
                description = description_list[qid]
                type_name = type_name_list[qid]
                image_path = image_path_list[qid]
                modelA_answer = ref_answer_modelA[qid]
                modelB_answer = answer_modelB[qid]
                prompt = construct_gpt4_query(text_instruction=text_instruction,
                                                  image_descriptsion=description,
                                                  modelA_answer=modelA_answer,
                                                  modelB_answer=modelB_answer)
                tasks.append(executor.submit(eval_worker, str(prompt),modelA,modelB,qid, type_name,image_path))
                
                
            pb = tqdm.tqdm(total=len(tasks))
            
            for i, future in enumerate(concurrent.futures.as_completed(tasks)):
                pb.update(1)
                try:   
                    new_data_item = future.result()
                    reviews.append(new_data_item)
                except Exception as e:
                    print(f"@@@ Exception: {e}\n")
            
            sum_score = 0
            sum_cnt = 0
            for item in reviews:
                sum_score += item['score']
                sum_cnt += 1
            print(f'Score is {sum_score / sum_cnt:.3f}')
            
            current_datetime = datetime.now()
            formatted_datetime = current_datetime.strftime('%m-%d-%H-%M')
            
            json.dump(reviews, open(args.answer_modelB + formatted_datetime, 'w'),
                      indent=2, ensure_ascii=False)
